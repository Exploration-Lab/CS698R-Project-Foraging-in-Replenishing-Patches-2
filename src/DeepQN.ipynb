{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "name": "Project_DeepQN",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOHeMWdl5723"
      },
      "source": [
        "%matplotlib inline\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.4\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 100\n",
        "TARGET_UPDATE = 4\n",
        "steps_done = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Ej0vq16FXi"
      },
      "source": [
        "# change the directory to environment directory \n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#%cd /content/drive/MyDrive/Foraging_in_replenishing_patches-master (4)/Foraging_in_replenishing_patches-master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdAlZUHW8y0"
      },
      "source": [
        "!pip3 install -e gym-env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGERJAOzrsyP"
      },
      "source": [
        "Restart the Kernal Now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxVY20DHrz6l"
      },
      "source": [
        "# change the directory to environment directory \n",
        "#%cd /content/drive/MyDrive/Foraging_in_replenishing_patches-master (4)/Foraging_in_replenishing_patches-master"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csxcIePL5728"
      },
      "source": [
        "# This notebook was modified from the sample documented code available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "env = gym.make('CartPole-v0').unwrapped\n",
        "\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5zVFo_v573A"
      },
      "source": [
        "# Implementation of replay memory buffer\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(Transition(*args))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RywAEmsc573C"
      },
      "source": [
        "# Maps states values to action values \n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN,self).__init__() ## calls __init__ method of nn.Module class\n",
        "        self.fc1= nn.Linear(state_size,128)\n",
        "        self.fc2 = nn.Linear(128,64)\n",
        "        self.fc3 = nn.Linear(64,action_size)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return F.relu(self.fc3(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpwgCBy6573H"
      },
      "source": [
        "n_actions = env.action_space.n\n",
        "policy_net = DQN(10, n_actions).to(device)\n",
        "target_net = DQN(10, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "optimizer = optim.RMSprop(policy_net.parameters(),lr=0.0001)\n",
        "memory = ReplayMemory(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-f2Xghxfj1"
      },
      "source": [
        "def select_action(state_a,eps):\n",
        "  # select an action based on current state and epsilon randomness\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = eps\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return (torch.argmax(policy_net(state_a)).unsqueeze(0)).unsqueeze(0)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF0pPF_v573I"
      },
      "source": [
        "def optimize_model():\n",
        "  # optimizes the model by updating the neural net\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    batch = Transition(*zip(*transitions))\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    state_action_values = policy_net(state_batch.reshape(128,10)).gather(1, action_batch)\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states.reshape(128,10)).max(1)[0].detach()\n",
        "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "    criterion = nn.MSELoss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    for param in policy_net.parameters():\n",
        "        param.grad.data.clamp_(-1, 1)\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SvOYrfI8M4K"
      },
      "source": [
        "def make_array(state,r):\n",
        "  # takes states and reward and convert it into an array with one-hot encoded states followed by rewards \n",
        "  to_state = [0,0,0,0,0,0,0,0,0,0]\n",
        "  to_state[state]=1\n",
        "  to_state[9]=r/100\n",
        "  to_state =reward = torch.tensor(to_state, device=device)\n",
        "  return to_state "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbVRR6GpMPMb"
      },
      "source": [
        "def playEnv(env,policy_net):  \n",
        "  # code to finally play with the environment\n",
        "    env.reset()\n",
        "    s = env.present_state\n",
        "    done = False\n",
        "    a = int(np.argmax(policy_net(make_array(s,0)).detach().numpy()))\n",
        "\n",
        "    cur_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        out = env.step(a)\n",
        "        (s_prime,t),r,done,_ = out\n",
        "        cur_reward += r\n",
        "        a = int(np.argmax(policy_net(make_array(s_prime,r)).detach().numpy()))\n",
        "        \n",
        "    return cur_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EEYwf8AOE1"
      },
      "source": [
        "from blocks import EnvClass\n",
        "envs = EnvClass()\n",
        "env = envs.getBlock(1) # change type of block address from here "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3-RP0HV573I"
      },
      "source": [
        "num_episodes = 10000\n",
        "eps = 1 \n",
        "result = []\n",
        "result_play = []\n",
        "for i_episode in range(num_episodes):\n",
        "    score = 0 \n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    eps = max(eps * 0.995,0.1) \n",
        "    state = make_array(8,0) \n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state,eps)\n",
        "        next_state_, reward, done, _ = env.step(int(action[0][0]))\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        next_state = make_array(next_state_[0],reward)\n",
        "        score+=reward\n",
        "\n",
        "        # Store the transition in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the policy network)\n",
        "        optimize_model()\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            break\n",
        "            \n",
        "    # Update the target network, copying all weights and biases in DQN\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "    t = playEnv(env,policy_net)\n",
        "    result.append(score)\n",
        "    result_play.append(t)\n",
        "\n",
        "print('Complete')\n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwOrw3xxMqPZ"
      },
      "source": [
        "playEnv(env,policy_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8JCyyJOJnwM"
      },
      "source": [
        "r = np.array(result)\n",
        "with open('deepqn.npy', 'wb') as f:\n",
        "    np.save(f, r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dEM3tMY1j38"
      },
      "source": [
        "import numpy as np\n",
        "a = np.load(\"/content/q_learning_ucb_B3_policy.npz\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG7IirgU1svf",
        "outputId": "bfac2d3a-1ce9-4a22-95b2-61dbc83fd463"
      },
      "source": [
        "len(a['arr_0'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lP4bXazx1tTy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
